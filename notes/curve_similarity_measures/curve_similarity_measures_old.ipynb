{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Curve Similarity Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Suppose we have a curve representation method, for example some parameterization scheme such as a\n",
    "spline. We also have a particular shape specified (say the Stanford Bunny) and we want our\n",
    "parameterization to represent that shape as closely as possible. To represent the given shape one\n",
    "way would be to tune the parameters of the representation scheme iteratively using a gradient\n",
    "descent optimization approach. This requires us to define an objective function that can then be\n",
    "optimized over. An appropriate objective for this task would be some measure of\n",
    "similarity(or dissimilarity) between the target curve and the one traced out by our parameterization\n",
    "which can then be maximized(or minimized) to fit the parameters.\n",
    "\n",
    "The target of this tutorial is to study **curve similarity measures**. We discuss different kinds of\n",
    "measures and also see their implementations.\n",
    "\n",
    "<p style=\"text-align:center;\"><img src=\"a_basic_spline.svg\" alt=\"a basic spline\" width=\"25%\"></p>\n",
    "<p style=\"text-align:center;\"><img src=\"stanford_bunny.svg\" alt=\"stanford bunny\" width=\"15%\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concrete Problem Setup\n",
    "First we define the different objects that we deal with:\n",
    "- **_Shape parameterization_**: This is the parameterization scheme that we use to represent our\n",
    "shapes. We have a set of parameters $\\phi$ that represent our shape. By changing $\\phi$ we trace out\n",
    "different curves in the plane. We will think of $\\phi$ as a column vector\n",
    "$[\\phi_1, \\phi_2, \\ldots, \\phi_n]^{T}$.\n",
    "\n",
    "- **_Parameterized Curve_**: This is the curve that is traced out by the parameterization scheme. We\n",
    "denote it by $C_p$ and is obtained by sampling the scheme at different points along the actual\n",
    "curve. It is specified in the form of an $N_p$ length sequence of $(x, y)$ points. These points are\n",
    "ordered along the curve. We will specify the points in a matrix in $\\mathbb{R}^{N_p \\times 2}$ where\n",
    "each row corresponds to a point $(x, y)$. We denote the matrix as $X_p$.\n",
    "\n",
    "- **_Target Curve_**: This is the curve that we want our parameterization scheme to represent. We\n",
    "denote it by $C_t$ and it is specified in the form of a $N_t$ length sequence of $(x, y)$ points.\n",
    "These points are ordered along the curve. We will specify the points in a matrix in\n",
    "$\\mathbb{R}^{N_t \\times 2}$ as with the parameterized curve. We denote the matrix as $X_t$.\n",
    "\n",
    "- **_Loss function_**: A function denoted as $\\mathcal{L}(X_t, X_p)$ that measures the degree of\n",
    "dissimilarity between the target curve and the parameterized curve. It should be differentiable to\n",
    "allow us to find gradients $\\frac{d\\mathcal{L}}{d\\phi}$ that can then be used to run gradient\n",
    "descent.\n",
    "\n",
    "**_Goal_**: To tune $\\phi$ such that our representation scheme traces out the target curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Measures\n",
    "We now discuss the different curve similarity measures. For each measure we describe the exact\n",
    "mathematical definition, practical considerations, modifications to make them differentiable and\n",
    "implementations in pytorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Squared Error (MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Description\n",
    "**_Assumption_**: $N_p = N_t = N$. That is, we sample the parameterized curve at exactly $N_t$\n",
    "points.\n",
    "\n",
    "The mean squared error loss function computes the average of the squared distance between the\n",
    "corresponding points on the two curves. Mathematically,\n",
    "$$\n",
    "\\mathcal{L} = \\frac{1}{N} \\sum_{i=1}^{N} \\left( d(X_{p}^{i}, X_{t}^{i}) \\right)^2\n",
    "$$\n",
    "where, $d$ is a distance function.\n",
    "\n",
    "Though the measure is quite naive and not very robust, it is very simple and quick to implement and\n",
    "is also differentiable without any modifications.\n",
    "\n",
    "<p style=\"text-align:center;\"><img src=\"mse_visualization.svg\" alt=\"stanford bunny\" width=\"35%\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "def mse_loss(X_p, X_t):\n",
    "    # Calculate the squared Euclidean distances between corresponding rows\n",
    "    squared_distances = torch.sum((X_p - X_t) ** 2, dim = 1)\n",
    "\n",
    "    # Calculate mean of the squared distances to get the loss\n",
    "    loss = torch.mean(squared_distances)\n",
    "\n",
    "    return loss\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fourier Descriptor Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Description\n",
    "The idea behind Fourier descriptor matching is to compute the Fourier coefficients of both the\n",
    "target and the parameterized curve and then use the difference between them as the loss function.\n",
    "\n",
    "Concretely, given a curve we can approximate it using a complex Fourier series as follows:\n",
    "$$\n",
    "X(t) = \\sum_{n = -\\infty}^{\\infty} c_n e^{n 2 \\pi i t} \\quad t \\in [0, 1)\n",
    "$$\n",
    "\n",
    "In Fourier descriptor matching we use the FFT algorithm to compute a finite number of coefficients\n",
    "$c_n$ for each of the curves which have themselves been sampled(from $X(t)$) at a finite number of\n",
    "points given by $X_p$ and $X_t$. Let the coefficients be defined in vectors $F_p$ and $F_t$. The\n",
    "loss is then computed as the mean squared error of the two coefficient vectors. If $k$ is the total\n",
    "Fourier coefficients computed in each FFT then the loss is given by:\n",
    "$$\n",
    "\\mathcal{L} = \\frac{1}{k} \\sum_{i=1}^{k} \\left( d(F_{p}^{i}, F_{t}^{i}) \\right)^2\n",
    "$$\n",
    "\n",
    "**_Note 1_**: Fourier Descriptor Matching works only for **closed curves**.\n",
    "\n",
    "**_Note 2_**: For the loss to be differentiable we require that the FFT be computed in a way that\n",
    "allows automatic differentiation to work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.fft\n",
    "\n",
    "def fourier_descriptor_matching_loss(X_p, X_t, num_descriptors):\n",
    "    # Compute Fourier transforms (using FFT)\n",
    "    fft1 = torch.fft.fft(torch.complex(X_p[..., 0], X_p[..., 1]), dim=0)\n",
    "    fft2 = torch.fft.fft(torch.complex(X_t[..., 0], X_t[..., 1]), dim=0)\n",
    "\n",
    "    # Select relevant descriptors (low frequencies)\n",
    "    descriptors1 = fft1[:num_descriptors]\n",
    "    descriptors2 = fft2[:num_descriptors]\n",
    "\n",
    "    # Calculate MSE loss on magnitudes or complex values\n",
    "    loss = torch.mean(torch.abs(descriptors1 - descriptors2)**2)\n",
    "    return loss\n",
    "```\n",
    "\n",
    "The implementation works because the FFT is differentiable in pytorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hausdorff Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Description\n",
    "**_Note_**: Most of the information is taken from the Wikipedia page\n",
    "[Hausdorff Distance](https://en.wikipedia.org/wiki/Hausdorff_distance).\n",
    "\n",
    "The Hausdorff distance measures how far two subsets of a metric space are from each other.\n",
    "Informally, two sets are close in the Hausdorff distance if every point of either set is close to\n",
    "some point of the other set. The Hausdorff distance is the longest distance someone can be forced to\n",
    "travel by an adversary who chooses a point in one of the two sets, from where they then must travel\n",
    "to the other set. In other words, it is the greatest of all the distances from a point in one set to\n",
    "the closest point in the other set.\n",
    "\n",
    "Let $(M, d)$ be a metric space. For each pair of non-empty subsets $X \\subset M$ and $Y \\subset M$,\n",
    "the Hausdorff distance between $X$ and $Y$ is defined as\n",
    "$$\n",
    "d_{\\mathrm H}(X,Y) := \\max\\left\\{\\,\\sup_{x \\in X} d(x,Y),\\ \\sup_{y \\in Y} d(X,y) \\,\\right\\}\n",
    "$$\n",
    "\n",
    "where $\\sup$ represents the supremum operator, $\\inf$ the infimum operator, and where\n",
    "$d(a, B) := \\inf_{b \\in B} d(a,b)$ quantifies the distance from a point $a \\in X$ to the subset $B\n",
    "\\subseteq X$.\n",
    "\n",
    "<p style=\"text-align:center;\"><img src=\"Hausdorff_distance.svg\" alt=\"stanford bunny\" width=\"25%\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Differentiability\n",
    "The Hausdorff distance is by itself not differentiable as we would implement using minimum and\n",
    "maximum functions. Therefore we need to work with approximations to it such as:\n",
    "- **_Soft Hausdorff_**: Compute the approximate Hausdorff distance by smoothing the minimum\n",
    "operation to ensure differentiability. In this approach we use a smooth minimum function instead of\n",
    "the minimum function directly.\n",
    "  1. Let $P_1$ and $P_2$ be the sets of points on the two curves.\n",
    "  2. Use a soft-minimum function to approximate the minimum distance between points, such as:\n",
    "$$\n",
    "d_{\\text{soft}}(p, P_2) = -\\log \\left( \\sum_{q \\in P_2} \\exp \\left( -\\frac{\\|p - q\\|_2^2}{\\tau} \\right) \\right)\n",
    "$$\n",
    "where $\\tau$, the temperature parameter, controls the sharpness of the approximation. As $\\tau$\n",
    "approaches 0, the softmin approaches the true minimum.\n",
    "  3. Compute the smoothed Hausdorff distance:\n",
    "$$\n",
    "\\text{Hausdorff}_{\\text{soft}}(P_1, P_2) = \\frac{1}{|P_1|} \\sum_{p \\in P_1} d_{\\text{soft}}(p, P_2) + \\frac{1}{|P_2|} \\sum_{q \\in P_2} d_{\\text{soft}}(q, P_1)\n",
    "$$\n",
    "- **_Relaxed Hausdorff_**: Another approach is to consider the average distance to the $k$ nearest\n",
    "neighbors instead of just the single nearest neighbor. This provides some smoothing.\n",
    "\n",
    "The LogSumExp (LSE) function is a smooth maximum – a smooth approximation to the maximum function.\n",
    "It is defined as the logarithm of the sum of the exponentials of the arguments:\n",
    "$$\n",
    "\\mathrm{LSE}(x_1, \\ldots, x_n) = \\log\\left( \\exp(x_1) + \\cdots + \\exp(x_n) \\right)\n",
    "$$\n",
    "\n",
    "Writing $\\mathbf{x} = (x_1, \\ldots, x_n)$ the partial derivatives are:\n",
    "$$\n",
    "\\frac{\\partial}{\\partial x_i}{\\mathrm{LSE}(\\mathbf{x})} = \n",
    "\\frac{\\exp x_i}{\\sum_j \\exp {x_j}}\n",
    "$$\n",
    "which means the gradient of LogSumExp is the softmax function.\n",
    "\n",
    "Also, note that $\\min \\left( x, y \\right) = -\\max \\left(-x, -y \\right)$. We can use this to get the\n",
    "smooth minimum function using the LogSumExp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smoothed Hausdorff Distance: 13.095537185668945\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def smoothed_hausdorff_distance(P1, P2, sigma=1.0):\n",
    "    \"\"\"\n",
    "    Compute the smoothed Hausdorff distance between two sets of points P1 and P2.\n",
    "    \n",
    "    Args:\n",
    "        P1 (torch.Tensor): A tensor of shape (N1, D), where N1 is the number of points in P1.\n",
    "        P2 (torch.Tensor): A tensor of shape (N2, D), where N2 is the number of points in P2.\n",
    "        sigma (float): Controls the sharpness of the soft-minimum operation.\n",
    "    \n",
    "    Returns:\n",
    "        float: The smoothed Hausdorff distance.\n",
    "    \"\"\"\n",
    "    # Compute pairwise squared distances\n",
    "    dist_matrix = torch.cdist(P1, P2, p=2) ** 2  # Shape: (N1, N2)\n",
    "    \n",
    "    # Compute soft-minimum distances\n",
    "    d_soft_p1_to_p2 = -torch.logsumexp(-dist_matrix / sigma, dim=1)  # Shape: (N1,)\n",
    "    d_soft_p2_to_p1 = -torch.logsumexp(-dist_matrix.t() / sigma, dim=1)  # Shape: (N2,)\n",
    "    \n",
    "    # Average soft-min distances\n",
    "    hausdorff_p1_to_p2 = d_soft_p1_to_p2.mean()\n",
    "    hausdorff_p2_to_p1 = d_soft_p2_to_p1.mean()\n",
    "    \n",
    "    # Combine the two directions\n",
    "    hausdorff_soft = hausdorff_p1_to_p2 + hausdorff_p2_to_p1\n",
    "    \n",
    "    return hausdorff_soft.item()\n",
    "\n",
    "# Example usage\n",
    "P1 = torch.tensor([[0.0, 0.0], [1.0, 0.0], [0.5, 0.5]])\n",
    "P2 = torch.tensor([[0.0, 1.0], [1.0, 1.0]])\n",
    "sigma = 0.1\n",
    "\n",
    "distance = smoothed_hausdorff_distance(P1, P2, sigma)\n",
    "print(f\"Smoothed Hausdorff Distance: {distance}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def relaxed_hausdorff(set1, set2, k=3):\n",
    "    \"\"\"Computes a relaxed Hausdorff distance.\"\"\"\n",
    "    distances = torch.cdist(set1, set2) # pairwise distances\n",
    "    min_distances, _ = torch.topk(distances, k, dim=1, largest=False)\n",
    "    relaxed_dist1 = torch.mean(min_distances)\n",
    "\n",
    "    distances = torch.cdist(set2, set1)\n",
    "    min_distances, _ = torch.topk(distances, k, dim=1, largest=False)\n",
    "    relaxed_dist2 = torch.mean(min_distances)\n",
    "    return torch.max(relaxed_dist1, relaxed_dist2)\n",
    "\n",
    "# Example usage:\n",
    "set1 = torch.randn(100, 2) # 100 points in 2D\n",
    "set2 = torch.randn(150, 2)\n",
    "loss = relaxed_hausdorff(set1, set2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'base (Python 3.12.8)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n base ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "print('hello')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
